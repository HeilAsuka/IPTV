name: Run Scraping and Commit Changes

on:
  schedule:
    - cron: '0 10 * * *'  # UTC 时间上午10点（北京时间晚上6点）
  workflow_dispatch:  # 允许手动触发
jobs:
  scrape:
    runs-on: ubuntu-latest  # 使用 Ubuntu 最新版本的 GitHub Runner

    steps:
    - name: Checkout repository
      uses: actions/checkout@v2  # 克隆仓库

    - name: Set up Python environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'  # 可以根据需要选择合适的 Python 版本

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install --no-cache-dir -r requirements.txt
        pip install --no-cache-dir playwright
        python -m playwright install  # 安装 Playwright 需要的浏览器

    - name: Run the scraper
      run: |
        python main.py  # 运行爬虫脚本

    - name: Commit and push changes (unconditionally)
      run: |
        git config --global user.name "MemoryCollection"  # 设置 Git 提交的用户名
        git config --global user.email "csszue@gmail.com"  # 设置 Git 提交的邮箱
        git add .  # 添加所有文件的变化
        git status  # 输出 git 状态，确保有文件被添加
        git commit -m "Upload scraped data" || echo "No changes to commit"  # 如果没有变化则跳过提交
        git push origin main  # 强制推送到 main 分支
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # 使用 GitHub 提供的 token 进行身份认证
